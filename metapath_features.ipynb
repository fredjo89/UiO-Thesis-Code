{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "765c5839",
   "metadata": {},
   "source": [
    "# #1: Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d3628c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General \n",
    "import sys, numpy as np, pandas as pd, math, matplotlib.pyplot as plt, datetime, copy, os\n",
    "\n",
    "# Pytorch, pytorch Geometric\n",
    "import torch, torch_geometric\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import MetaPath2Vec\n",
    "\n",
    "# Sklearn\n",
    "import sklearn\n",
    "\n",
    "sys.path.insert(1, '/home/ec2-user/SageMaker/repos/fredriks-thesis/python')\n",
    "import helper_functions, graph_preprocessing, nn_models, hetero_models, graphSage_models, div_models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d5a56c",
   "metadata": {},
   "source": [
    "# #2: Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6168df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_path = \"/home/ec2-user/SageMaker/repos/fredriks-thesis/notebooks/09_model_performance_script/models/\"\n",
    "\n",
    "settings = {\n",
    "    'dataset': 1e4\n",
    "    ,'seed': 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc80805",
   "metadata": {},
   "source": [
    "# #3: Load Dataset and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e98e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/home/ec2-user/SageMaker/s3/exploration-876679093433-ew1-initiative-pop-amlanalysis/data/fredriks-thesis/heterographs_01/'\n",
    "filename = \"heterograph_externalnodes_{:.0f}.pt\".format(settings['dataset'])\n",
    "\n",
    "data = torch.load(filepath+filename)\n",
    "\n",
    "# Removing the attribute globalRiskScore\n",
    "data['ind'].x = torch.cat((data['ind'].x[:,0:4], data['ind'].x[:,5:data['ind'].x.shape[1]]), 1)\n",
    "data['org'].x = torch.cat((data['org'].x[:,0:3], data['org'].x[:,4:data['ind'].x.shape[1]]), 1)\n",
    "#data['ind'].attr_names.remove('globalRiskScore')\n",
    "#data['org'].attr_names.remove('globalRiskScore')\n",
    "\n",
    "torch.manual_seed(settings['seed']) # Setting torch random state seed\n",
    "\n",
    "# Create num_features variables\n",
    "data['ind'].num_features = data['ind'].x.shape[1]\n",
    "data['org'].num_features = data['org'].x.shape[1]\n",
    "data['ext'].num_features = data['ext'].x.shape[1]\n",
    "\n",
    "# Reversing all edges \n",
    "data = graph_preprocessing.reverse_edges(data)\n",
    "# Applying log to node feature transaction amounts and edge feature transaction amounts: \n",
    "data = graph_preprocessing.apply_log_to_txns(data)\n",
    "# Normalizing node features\n",
    "data = graph_preprocessing.normalize_node_features(data)\n",
    "# Scaling edge_attributes to be in range [0.01,1]\n",
    "data = graph_preprocessing.scaling_edge_attr(data)\n",
    "\n",
    "\n",
    "# Adding dummy-features for role-edges; ones for all edges\n",
    "data[('ind', 'role', 'org')].edge_attr = torch.ones([data[('ind', 'role', 'org')].edge_index.shape[1],1], dtype = torch.float32)\n",
    "data[('org', 'rev_role', 'ind')].edge_attr = torch.ones([data[('org', 'rev_role', 'ind')].edge_index.shape[1],1], dtype = torch.float32)\n",
    "\n",
    "# Define device and transfer data to device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7797f27",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61807ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 16\n",
    "batch_size = int(math.pow(2,13))\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbcb1be",
   "metadata": {},
   "source": [
    "# IND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abffcb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "metapaths = [\n",
    "[('ind', 'txn', 'org'),('org', 'txn', 'ind')],\n",
    "[('ind', 'txn', 'ext'),('ext', 'txn', 'ind')],\n",
    "[('ind', 'role', 'org'),('org', 'txn', 'ind')]\n",
    "]\n",
    "\n",
    "my_embeddings_ind = []\n",
    "\n",
    "for it in range(len(metapaths)):\n",
    "    print(\"Iteration: {}\".format(it))\n",
    "    my_filename = \"metapath_features_ind_{}_{:.0f}.pt\".format(it+1,settings['dataset'])\n",
    "    if os.path.isfile(filepath + my_filename):\n",
    "        print(\"Skipping Iteration: {}\".format(it))\n",
    "        continue\n",
    "\n",
    "    model = MetaPath2Vec(data.edge_index_dict, embedding_dim=embedding_dim,\n",
    "                         metapath=metapaths[it], walk_length=20, context_size=10,\n",
    "                         walks_per_node=10, num_negative_samples=1, \n",
    "                         sparse=True).to(device)\n",
    "\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    print(\"Number of train-batches: {}\".format(len(iter(loader))))\n",
    "\n",
    "\n",
    "    ## Train ##\n",
    "    start_time_total = helper_functions.stopwatch()\n",
    "\n",
    "    max_epochs = 500\n",
    "    check_progress_frequency = 1\n",
    "    train_hist = pd.DataFrame( columns = ['loss'])\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        ## Tracking Progress ##\n",
    "        train_hist.loc[epoch] = pd.Series({  \n",
    "            'loss': total_loss\n",
    "         })\n",
    "\n",
    "        # Early Stopping\n",
    "        if train_hist.loc[epoch].loss > min(train_hist.loss):\n",
    "            print(\"Early stopping at epoch {}\".format(epoch));\n",
    "            torch.save(model('ind').cpu(), filepath+my_filename)\n",
    "            break;\n",
    "\n",
    "        if epoch%check_progress_frequency == 0 or epoch==max_epochs-1:\n",
    "            tms = divmod((datetime.datetime.now() - start_time_total).days * 86400 + (datetime.datetime.now() - start_time_total).seconds, 60)  \n",
    "            print(\"Epoch #: {} finished, Loss: {:.2f}, Time Elapsed: {} min {} sek\".format(epoch, total_loss, tms[0], tms[1]))  \n",
    "\n",
    "\n",
    "    helper_functions.stopwatch(start_time_total)\n",
    "    helper_functions.sound_alert()\n",
    "\n",
    "    # Plotting loss curve\n",
    "    lw = 1\n",
    "    plt.figure(figsize = (5,5))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.plot(train_hist, label = 'loss', linewidth = lw, color = 'blue');\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    ax.set_yscale('log');\n",
    "    ax.legend();\n",
    "\n",
    "    my_embeddings_ind.append(model('ind'))\n",
    "\n",
    "helper_functions.sound_alert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722e499c",
   "metadata": {},
   "source": [
    "# ORG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bafa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import MetaPath2Vec\n",
    "\n",
    "metapaths = [\n",
    "[('org', 'txn', 'ind'),('ind', 'txn', 'org')],\n",
    "[('org', 'txn', 'ext'),('ext', 'txn', 'org')],\n",
    "[('org', 'txn', 'ind'),('ind', 'role', 'org')]\n",
    "]\n",
    "\n",
    "my_embeddings_org = []\n",
    "\n",
    "for it in range(len(metapaths)):\n",
    "    print(\"Iteration: {}\".format(it))\n",
    "    my_filename = \"metapath_features_org_{}_{:.0f}.pt\".format(it+1,settings['dataset'])\n",
    "    if os.path.isfile(filepath + my_filename):\n",
    "        print(\"Skipping Iteration: {}\".format(it))\n",
    "        continue\n",
    "\n",
    "\n",
    "    model = MetaPath2Vec(data.edge_index_dict, embedding_dim=embedding_dim,\n",
    "                         metapath=metapaths[it], walk_length=20, context_size=10,\n",
    "                         walks_per_node=10, num_negative_samples=1, \n",
    "                         sparse=True).to(device)\n",
    "\n",
    "    optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.01)\n",
    "    loader = model.loader(batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    print(\"Number of train-batches: {}\".format(len(iter(loader))))\n",
    "\n",
    "\n",
    "    ## Train ##\n",
    "    start_time_total = helper_functions.stopwatch()\n",
    "\n",
    "    max_epochs = 500\n",
    "    check_progress_frequency = 1\n",
    "    train_hist = pd.DataFrame( columns = ['loss'])\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for pos_rw, neg_rw in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "\n",
    "        ## Tracking Progress ##\n",
    "        train_hist.loc[epoch] = pd.Series({  \n",
    "            'loss': total_loss\n",
    "         })\n",
    "\n",
    "        # Early Stopping\n",
    "        if train_hist.loc[epoch].loss > min(train_hist.loss):\n",
    "            print(\"Early stopping at epoch {}\".format(epoch));\n",
    "            torch.save(model('org').cpu(), filepath+my_filename)\n",
    "            break;\n",
    "\n",
    "        if epoch%check_progress_frequency == 0 or epoch==max_epochs-1:\n",
    "            tms = divmod((datetime.datetime.now() - start_time_total).days * 86400 + (datetime.datetime.now() - start_time_total).seconds, 60)  \n",
    "            print(\"Epoch #: {} finished, Loss: {:.2f}, Time Elapsed: {} min {} sek\".format(epoch, total_loss, tms[0], tms[1]))  \n",
    "\n",
    "\n",
    "    helper_functions.stopwatch(start_time_total)\n",
    "    helper_functions.sound_alert()\n",
    "\n",
    "    # Plotting loss curve\n",
    "    lw = 1\n",
    "    plt.figure(figsize = (5,5))\n",
    "    ax = plt.subplot(1, 1, 1)\n",
    "    ax.plot(train_hist, label = 'loss', linewidth = lw, color = 'blue');\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    ax.set_yscale('log');\n",
    "    ax.legend();\n",
    "\n",
    "    my_embeddings_org.append(model('org'))\n",
    "\n",
    "helper_functions.sound_alert()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_geometric_env",
   "language": "python",
   "name": "conda_pytorch_geometric_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
